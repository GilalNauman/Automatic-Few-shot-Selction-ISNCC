{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "41a6777d5e67dc652f57ce9681b4c44dc44152be",
    "colab_type": "text",
    "id": "uNaVQGQ9tQRr"
   },
   "source": [
    "<h1><center><b>Tropical Fishes</b></center></h1>\n",
    "<h1><center><b>Baseline Training</b></center></h1>\n",
    "<center><b>15 March 2023</b></center><br/>\n",
    "<center><b>ISNCC IEEE @HBKU</b></center><br/>\n",
    "<center><b>Nauman Ullah Gilal</b></center><br/>\n",
    "<center><b>Co-Authors: Fahad Majeed, Khaled Al-Thelaya, Mehak Khan, Jens Schneider, and Marco Agus</b></center><br/>\n",
    "<center><b>IDEAL Lab</b></center><br/>\n",
    "<center><b>College of Science and Engineering</b></center><br/>\n",
    "<center><b>Hamad Bin Khalifa University<b></center><br/></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "Check training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls scrapped_dataset_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:24.694292Z",
     "start_time": "2021-11-26T12:31:24.526520Z"
    },
    "_uuid": "7ceb7f04d55f3d1510a03868713e773e7df17046",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "7wJ_OH1DQyrd",
    "outputId": "bda2768c-24b9-4962-b7eb-fb3b07d6fad4"
   },
   "outputs": [],
   "source": [
    "!ls test\n",
    "\n",
    "# train_path = '/workspace/Nauman/MENA/MENA-v1-4C-Gym-Scheme/MENA-v1-c-109/MENA-v1-c-109/Training'\n",
    "# test_path = '/workspace/Nauman/MENA/MENA-v1-4C-Gym-Scheme/MENA-v1-c-109/MENA-v1-c-109/Testing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set Categories, Classes, Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:24.701524Z",
     "start_time": "2021-11-26T12:31:24.696013Z"
    },
    "_uuid": "9a3e091f8d5db4be5a553a2fd23970dde7c649f2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1855
    },
    "colab_type": "code",
    "id": "yy_pAK35Rbdi",
    "outputId": "9b374f07-961a-4878-85a2-86589b2f68cb",
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "categories = os.listdir('scrapped_dataset_new')\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:24.704167Z",
     "start_time": "2021-11-26T12:31:24.702349Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"scrapped_dataset_new\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:25.951403Z",
     "start_time": "2021-11-26T12:31:24.704976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the training data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "rows = 2\n",
    "cols = 5\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(60,60))\n",
    "fig.suptitle(\"Showing one random image from each training class\", y=1.05, fontsize=40, weight = 'bold') # Adding  y=1.05, fontsize=24 helped me fix the suptitle overlapping with axes issue\n",
    "data_dir = TRAIN_DIR\n",
    "foods_sorted = sorted(os.listdir(data_dir))\n",
    "food_id = 0\n",
    "for i in range(rows):\n",
    "  for j in range(cols):\n",
    "    try:\n",
    "      food_selected = foods_sorted[food_id] \n",
    "      food_id += 1\n",
    "    except:\n",
    "      break\n",
    "    if food_selected == '.DS_Store':\n",
    "        continue\n",
    "    food_selected_images = os.listdir(os.path.join(data_dir,food_selected)) # returns the list of all files present in each food category\n",
    "    food_selected_random = np.random.choice(food_selected_images) # picks one food item from the list as choice, takes a list and returns one random item\n",
    "    img = plt.imread(os.path.join(data_dir,food_selected, food_selected_random))\n",
    "    ax[i][j].imshow(img)\n",
    "    ax[i][j].set_title(food_selected, pad = 10,weight = 'bold', color = 'red', fontsize = 30)\n",
    "    \n",
    "plt.setp(ax, xticks=[],yticks=[])\n",
    "plt.tight_layout()\n",
    "# https://matplotlib.org/users/tight_layout_guide.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of samples from Test set 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "rows = 2\n",
    "cols = 5\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(60,60))\n",
    "fig.suptitle(\"Showing one random image from each training class\", y=1.05, fontsize=40, weight = 'bold') # Adding  y=1.05, fontsize=24 helped me fix the suptitle overlapping with axes issue\n",
    "data_dir = 'test'\n",
    "foods_sorted = sorted(os.listdir(data_dir))\n",
    "food_id = 0\n",
    "for i in range(rows):\n",
    "  for j in range(cols):\n",
    "    try:\n",
    "      food_selected = foods_sorted[food_id] \n",
    "      food_id += 1\n",
    "    except:\n",
    "      break\n",
    "    if food_selected == '.DS_Store':\n",
    "        continue\n",
    "    food_selected_images = os.listdir(os.path.join(data_dir,food_selected)) # returns the list of all files present in each food category\n",
    "    food_selected_random = np.random.choice(food_selected_images) # picks one food item from the list as choice, takes a list and returns one random item\n",
    "    img = plt.imread(os.path.join(data_dir,food_selected, food_selected_random))\n",
    "    ax[i][j].imshow(img)\n",
    "    ax[i][j].set_title(food_selected, pad = 10,weight = 'bold', color = 'green', fontsize = 30)\n",
    "    \n",
    "plt.setp(ax, xticks=[],yticks=[])\n",
    "plt.tight_layout()\n",
    "# https://matplotlib.org/users/tight_layout_guide.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaration of Efficient Net family with Resolution, Epochs, Learning Rate, Batch size and Label smoothing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.235114Z",
     "start_time": "2021-11-26T12:31:28.215051Z"
    }
   },
   "outputs": [],
   "source": [
    "architecture = 'efficientnet-b0'\n",
    "logfile = 'efficientnet-b0-MENA.csv'\n",
    "\n",
    "## The tables\n",
    "effnet_size = ({\n",
    "    'efficientnet-b0':300,      # original resolution\n",
    "    'efficientnet-b1':240,      # original resolution\n",
    "    'efficientnet-b2':400,      # original resolution\n",
    "    'efficientnet-b3':400,      # original resolution\n",
    "    'efficientnet-b4':500,      # original resolution\n",
    "    'efficientnet-b5':456,      # original resolution\n",
    "    'efficientnet-b6':528,      # original resolution\n",
    "    'efficientnet-b7':600,      # original resolution\n",
    "    'efficientnet-lite0':224,\n",
    "    'efficientnet-lite2':260,\n",
    "    'efficientnet-lite4':384,\n",
    "    'resnet18':224,\n",
    "    'resnet34':224,\n",
    "    'resnet50':224,\n",
    "    'resnet101':224,\n",
    "    'resnet152':224\n",
    "})\n",
    "batch_size = ({ \n",
    "    'efficientnet-b0':128,      # initially 160\n",
    "    'efficientnet-b1':128,      #\n",
    "    'efficientnet-b2':128,       #88 160\n",
    "    'efficientnet-b3':16,       #\n",
    "    'efficientnet-b4':16,       #\n",
    "    'efficientnet-b5':14,       #\n",
    "    'efficientnet-b6':16,\n",
    "    'efficientnet-b7':8,\n",
    "    'efficientnet-lite0': 160,\n",
    "    'efficientnet-lite2': 160,\n",
    "    'efficientnet-lite4':24,\n",
    "    \n",
    "    'resnet18':400,             # 3080: ok\n",
    "    'resnet34':320,            # RTX 3090 rocz!\n",
    "    'resnet50':256,             # 3080: ok\n",
    "    'resnet101':96,\n",
    "    'resnet152':64\n",
    "})\n",
    "learning_rate = ({\n",
    "    'efficientnet-b0':5e-4,     # 2e-3: too large, 5e-4: too large\n",
    "    'efficientnet-b1':2e-3,     # 1e-3: 82.53/93.9\n",
    "    'efficientnet-b2':1e-3,     # 1e-3, 2e-3     \n",
    "    'efficientnet-b3':2e-3,    \n",
    "    'efficientnet-b4':2e-3,  \n",
    "    'efficientnet-b5':1.5e-3,  \n",
    "    'efficientnet-b6':4e-3,\n",
    "    'efficientnet-b7':4e-3,\n",
    "    'efficientnet-lite0': 5e-4,\n",
    "    'efficientnet-lite2':1e-3,\n",
    "    'efficientnet-lite4':5e-4,\n",
    "    'resnet18':2.823e-02,\n",
    "    'resnet34':-1,\n",
    "    'resnet50':1.016e-02,\n",
    "    'resnet101':-1,\n",
    "    'resnet152':-1,\n",
    "})\n",
    "epochs = ({\n",
    "    'efficientnet-b0':(1,100), # for initally top losses\n",
    "    'efficientnet-b1':(6,20),\n",
    "    'efficientnet-b2':(15,50),\n",
    "    'efficientnet-b4':(5,25),\n",
    "    'efficientnet-lite0': (15,80),\n",
    "    'efficientnet-lite2': (5,25),\n",
    "    'resnet18':(10,50),\n",
    "    'resnet34':(10,50),\n",
    "    'resnet50':(10,50),\n",
    "    'resnet101':(10,50),\n",
    "    'resnet152':(10,50)\n",
    "})\n",
    "label_smoothing = ({           # a = 0.00     a = 0.05     a = 0.10     a = 0.15     a = 0.20\n",
    "    'efficientnet-b0':0.05,    #\n",
    "    'efficientnet-b1':0.05,    #\n",
    "    'efficientnet-b2':0.05,    # 0.05\n",
    "    'efficientnet-b3':0.05,    #\n",
    "    'efficientnet-b4':0.05,    #\n",
    "    'efficientnet-b5':0.05,\n",
    "    'efficientnet-b6':0.05,\n",
    "    'efficientnet-b7':0.05,\n",
    "    'efficientnet-lite0': 0.05,\n",
    "    'efficientnet-lite2': 0.05,\n",
    "    'resnet18':0.05,\n",
    "    'resnet34':0.05,\n",
    "    'resnet50':0.05,\n",
    "    'resnet101':0.05,\n",
    "    'resnet152':0.05\n",
    "})\n",
    "## Testing accuracies (top1,top5)\n",
    "scores = ({\n",
    "    'efficientnet-b0':(0,0),\n",
    "    'efficientnet-b1':(0,0),\n",
    "    'efficientnet-b2':(0,0),\n",
    "    'efficientnet-b3':(0,0),\n",
    "    'efficientnet-b4':(0,0),\n",
    "    'efficientnet-lite0':(0,0),\n",
    "    'efficientnet-lite2': (0,0),\n",
    "    'resnet50':(0,0)\n",
    "})\n",
    "tta_scores = ({\n",
    "    'efficientnet-b0':(0,0),\n",
    "    'efficientnet-b1':(0,0),\n",
    "    'efficientnet-b2':(0,0),\n",
    "    'efficientnet-b3':(0,0),\n",
    "    'efficientnet-b4':(0,0),\n",
    "    'efficientnet-lite0':(0,0),\n",
    "    'efficientnet-lite2': (0,0),\n",
    "    'efficientnet-lite4':(0,0),\n",
    "    'resnet50':(0,0)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Writing training data into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.249071Z",
     "start_time": "2021-11-26T12:31:28.235728Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os.path\n",
    "train_dir = \"scrapped_dataset_new\"\n",
    "training_path = Path(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.288929Z",
     "start_time": "2021-11-26T12:31:28.249683Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filepaths = list(training_path.glob(r'**/*.jpg'))\n",
    "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "images = pd.concat([filepaths, labels], axis=1)\n",
    "train_df = pd.DataFrame(images)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viualization of Training Data, Samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "lbl = train_df['Label']\n",
    "# print(lbl)\n",
    "# print(lbl.size)\n",
    "plt.figure(figsize=(70,20))\n",
    "ax = sns.countplot(x= lbl, data=train_df)\n",
    "ax.bar_label(ax.containers[0], weight = 'bold', fontsize = '30', color = 'red')\n",
    "plt.xticks(rotation=90, fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.yticks( fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.xlabel(\"Labels\", fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.ylabel(\"Number of Images\", fontsize = 40, weight = 'bold', color = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images Per Catergory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.304212Z",
     "start_time": "2021-11-26T12:31:28.290074Z"
    }
   },
   "outputs": [],
   "source": [
    "counted = train_df.groupby([\"Label\"]).size()\n",
    "print(counted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set with 50 Images per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.324789Z",
     "start_time": "2021-11-26T12:31:28.304863Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dir = \"test\"\n",
    "test_path = Path(test_dir)\n",
    "filepaths = list(test_path.glob(r'**/*.jpg'))\n",
    "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "images = pd.concat([filepaths, labels], axis=1)\n",
    "test_df= pd.DataFrame(images)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "lbl = test_df['Label']\n",
    "# print(lbl)\n",
    "# print(lbl.size)\n",
    "plt.figure(figsize=(70,20))\n",
    "ax = sns.countplot(x= lbl, data=test_df)\n",
    "ax.bar_label(ax.containers[0], weight = 'bold', fontsize = '30', color = 'red')\n",
    "plt.xticks(rotation=90, fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.yticks( fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.xlabel(\"Labels\", fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.ylabel(\"Number of Images\", fontsize = 40, weight = 'bold', color = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Fastai and pytorch Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.609896Z",
     "start_time": "2021-11-26T12:31:28.325404Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai import __version__\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from fastai.vision import *\n",
    "from fastai import optimizer, losses, metrics\n",
    "from functools import partial, wraps\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastai version checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "fastai.__version__\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch version checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.device(0)\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # batch size\n",
    "# bs = 128\n",
    "# # image resolution\n",
    "# img_size = 224\n",
    "model = EfficientNet.from_pretrained(\"efficientnet-b0\", num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of data for learner (Fastai learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the data augmentation \n",
    "# ds_tfms = ([RandTransform(tfm=RandomCrop (CropPad), kwargs={'row_pct': (0, 1), 'col_pct': (0, 1), 'padding_mode': 'reflection'}, p=1.0, resolved={}, do_run=True, is_random=True), \n",
    "#             RandTransform(tfm=TfmCoord (symmetric_warp), kwargs={'magnitude': (-0.2, 0.2)}, p=0.75, resolved={}, do_run=True, is_random=True), \n",
    "#             RandTransform(tfm=TfmAffine (rotate), kwargs={'degrees': (-40, 40)}, p=0.75, resolved={}, do_run=True, is_random=True), \n",
    "#             RandTransform(tfm=TfmAffine (flip_affine), kwargs={}, p=0.5, resolved={}, do_run=True, is_random=True), \n",
    "#             RandTransform(tfm=TfmAffine (zoom), kwargs={'scale': (1.0, 1.4), 'row_pct': (0, 1), 'col_pct': (0, 1)}, p=0.75, resolved={}, do_run=True, is_random=True), \n",
    "#             RandTransform(tfm=TfmLighting (brightness), kwargs={'change': (0.35, 0.65)}, p=0.75, resolved={}, do_run=True, is_random=True), \n",
    "#             RandTransform(tfm=TfmLighting (contrast), kwargs={'scale': (0.7, 1.43)}, p=0.75, resolved={}, do_run=True, is_random=True),\n",
    "#             RandTransform(tfm=TfmCoord (jitter), kwargs={'magnitude': (-0.01, 0.01)}, p=0.3, resolved={}, do_run=True, is_random=True),\n",
    "#             RandTransform(tfm=TfmCoord (skew), kwargs={'direction': (0, 7), 'magnitude': (0.2)}, p=0.75, resolved={}, do_run=True, is_random=True),\n",
    "#             RandTransform(tfm=TfmAffine (squish), kwargs={'scale': (0.42, 2.4), 'row_pct': (0, 1), 'col_pct': (0, 1)}, p=0.75, resolved={}, do_run=True, is_random=True)], \n",
    "#            [RandTransform(tfm=TfmCrop (crop_pad), kwargs={}, p=1.0, resolved={}, do_run=True, is_random=True)\n",
    "#            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#81.01% with less aug\n",
    "# tfms = [Zoom (min_zoom=1.0, max_zoom=1.2, p=0.5, draw=None, draw_x=None,draw_y=None, size=None,mode='bilinear', pad_mode='reflection', batch=False,align_corners=False),\n",
    "#         Rotate (max_deg=45, p=0.5 , mode='bilinear', pad_mode='reflection',align_corners=False, batch=False),\n",
    "#         Warp (magnitude=0.2, p=0.75),\n",
    "#         Flip (p=0.5, draw=None, size=None,mode='bicubic', pad_mode='reflection', align_corners=True,batch=False),\n",
    "#         Brightness(max_lighting = 0.2, p = 0.75),\n",
    "#         Contrast  (max_lighting = 0.2, p = 0.75),\n",
    "#         Saturation(max_lighting = 0.2, p = 0.75)\n",
    "    \n",
    "#        ]\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "batch_tfms = [Zoom(),Rotate(), Flip(), Brightness(), Contrast(), Saturation()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# default augmentation\n",
    "bs   = batch_size[architecture]\n",
    "imgs = effnet_size[architecture]\n",
    "resize = (imgs*4)//3\n",
    "dls = (ImageDataLoaders.from_df(train_df, valid_pct=0.2,bs=bs,label_col=1,\n",
    "                                        shuffle_train=True,\n",
    "                                        item_tfms=Resize(imgs),\n",
    "                                        batch_tfms = batch_tfms\n",
    "                                  \n",
    "#                                         batch_tfms=aug_transforms(mult = 3,\n",
    "#                                                                   do_flip=True,\n",
    "#                                                                   flip_vert=False,\n",
    "#                                                                   xtra_tfms=tfms,\n",
    "# #                                                                   max_rotate=40.0, \n",
    "# #                                                                   min_zoom=1.0, \n",
    "# #                                                                   max_zoom=1.4,\n",
    "# #                                                                   max_lighting=0.35,\n",
    "# #                                                                   max_warp=0.2, \n",
    "# #                                                                   p_affine=0.75,\n",
    "# #                                                                   p_lighting=0.75, \n",
    "# #                                                                   xtra_tfms=tfms\n",
    "#                                                                   size=imgs, \n",
    "# #                                                                   mode='bilinear',\n",
    "# #                                                                   pad_mode='reflection', \n",
    "# #                                                                   align_corners=True, \n",
    "# #                                                                   batch=False,\n",
    "# #                                                                   min_scale=1.0))\n",
    "                                 ))\n",
    "\n",
    "print(\"Image size=\", imgs)\n",
    "print(\"Batch size=\", bs)\n",
    "print(\"Architecture=\", architecture)\n",
    "print(resize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # default augmentation  best 83.83 with mult\n",
    "# bs   = batch_size[architecture]\n",
    "# imgs = effnet_size[architecture]\n",
    "# resize = (imgs*4)//3\n",
    "# data0 = (ImageDataLoaders.from_df(train_df, valid_pct=0.2,bs=bs,label_col=1,\n",
    "#                                         shuffle_train=True,\n",
    "#                                         item_tfms=Resize(imgs),\n",
    "#                                         batch_tfms=aug_transforms(mult = 2,\n",
    "#                                                                   do_flip=True,\n",
    "#                                                                   flip_vert=False,\n",
    "#                                                                   max_rotate=10.0,\n",
    "#                                                                   max_zoom=1.1,\n",
    "#                                                                   max_warp=0.2,\n",
    "#                                                                   p_affine=0.75,\n",
    "#                                                                   max_lighting=0.2,\n",
    "#                                                                   p_lighting=0.75,\n",
    "#                                                                   xtra_tfms=None\n",
    "#             )))\n",
    "\n",
    "# print(\"Image size=\", imgs)\n",
    "# print(\"Batch size=\", bs)\n",
    "# print(\"Architecture=\", architecture)\n",
    "# print(resize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = losses.CrossEntropyLossFlat()\n",
    "model = EfficientNet.from_pretrained(architecture, num_classes=10)\n",
    "learner_type = Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:30.444803Z",
     "start_time": "2021-11-26T12:31:30.385475Z"
    }
   },
   "outputs": [],
   "source": [
    "best_pth = '1epoch'\n",
    "checkpoints = SaveModelCallback(fname=best_pth,monitor='accuracy',comp=np.greater, with_opt=True)\n",
    "# brainfreeze = BnFreeze()\n",
    "learn = ( learner_type(dls, model,metrics=[accuracy],loss_func=loss_func,\n",
    "                        cbs=[ShowGraphCallback(),checkpoints]).to_fp16())\n",
    "print(\"Best pth is=\", best_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we choose learning rates?\n",
    "If the learning rate is too slow, training will take a lot of time. If the learning rate is too high, \n",
    "we will be jumping around the minimum loss, getting farther and farther from the minimum and never reach it.\n",
    "So to start the experiment of finding a learning rate, we train the model while increasing the learning rate.\n",
    "Then we plot the loss against the learning rate, and stop when the loss starts to explode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot_lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_lr = 1e-4\n",
    "lr = 1e-3\n",
    "learn.fit_one_cycle(7 , lr_max = slice(low_lr, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfreeze the Learner\n",
    "fast.ai freezes the pre-trained part when we create the model. So up to this point, \n",
    "we only trained the last layer block. Next, we will unfreeze the pre-trained part and train the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_lr = 1e-4\n",
    "lr = 1e-3\n",
    "learn.fit_one_cycle(20, lr_max = slice(low_lr, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test(%) on Test set 1 (50 Images per class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = (ImageDataLoaders.from_df(test_df, valid_pct = 0.0, splitter=None, shuffle=False, label_col=1, item_tfms=Resize(imgs)))\n",
    "preds = learn.get_preds(dl=data_test)\n",
    "preds\n",
    "print(\"length of preds[1]\",len(preds[1]))\n",
    "acc= accuracy(preds[0], preds[1])\n",
    "print(\" BaselineTop-1 Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = {}\n",
    "log_preds, y  = learn.tta(dl=data_test)\n",
    "tta_acc = accuracy(log_preds, y)\n",
    "print(tta_acc)\n",
    "err[0] = (100.0, 100.0*(1.0-float(tta_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_test = ClassificationInterpretation.from_learner(learn, dl =data_test)\n",
    "inter_test.plot_confusion_matrix(figsize = (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH=\"models/tf_exp0_scrapped_dataset.p\"\n",
    "# torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving best pth with baseline for future using and for scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# source = 'models/'+best_pth+'.pth'\n",
    "# destination = 'models/baseline/'+best_pth\n",
    "# # Copy the file from source to destination\n",
    "# shutil.copy(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = (ImageDataLoaders.from_df(train_df, valid_pct = 0.0, bs=bs, splitter=None, shuffle=False, label_col=1, item_tfms=Resize(imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_interp = ClassificationInterpretation.from_learner(learn, dl = dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 0\n",
    "for batch in dls.train:\n",
    "    num_images += len(batch[0])\n",
    "print(\"Number of images:\", num_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Get the top losses and their corresponding indexes\n",
    "losses, idx = entire_interp.top_losses(len(train_df), largest = False)\n",
    "\n",
    "# Get the image paths, label, and loss values corresponding to the top losses\n",
    "image_paths_and_labels_losses = [(train_df.iloc[int(i)]['Filepath'], train_df.iloc[int(i)]['Label'], losses[j].item()) for j, i in enumerate(idx)]\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open('min_losses.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Filepath', 'Label', 'Loss'])\n",
    "    for path, label, loss in image_paths_and_labels_losses:\n",
    "        writer.writerow([path, label, loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "min_losses = pd.read_csv('min_losses.csv')\n",
    "min_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 100 samples per class with min losses from all the data\n",
    "# import pandas as pd\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = min_losses\n",
    "\n",
    "# Group the samples by category and select the 50 samples with the minimum loss\n",
    "min_loss_200_per_cat = df.loc[df.groupby('Label')['Loss'].nsmallest(200).index.get_level_values(1)]\n",
    "# condition = 0.00002  # change the condition to whatever value you want\n",
    "# fs_5_per_cat_after_ocr = df.loc[df.groupby('Label').apply(lambda x: x[x['los'] > condition].nsmallest(5, columns=['los'])).index.get_level_values(1)]\n",
    "\n",
    "\n",
    "# Save the selected DataFrame as a CSV file\n",
    "min_loss_200_per_cat.to_csv('min_losses_200_per_cat.csv', index=False)\n",
    "min_loss_200_per_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "\n",
    "# Helper method to split dataset into train and test folders\n",
    "def split_data(df:pd.core.frame.DataFrame=None,filepath=None,shuffle=True,perc=0.0):\n",
    "    def get_class(file):\n",
    "        return file.split('/')[-2].strip()\n",
    "    \n",
    "    if type(filepath)!=type(None):\n",
    "        files = [file for file in glob(filepath+'/**/*.jpg')]\n",
    "    else:\n",
    "        assert(type(df)!=type(None))\n",
    "        files = df['Filepath']\n",
    "        \n",
    "    classes = set([get_class(name) for name in files])\n",
    "    \n",
    "    D = { key : [] for key in classes }\n",
    "    for file in tqdm(files):\n",
    "        D[get_class(file)].append(file)\n",
    "    xt, xv = [],[]    \n",
    "    for key,val in D.items():\n",
    "        keys = [key for n in range(len(val))]\n",
    "        F = [(x,y) for x,y in zip(val,keys)]\n",
    "        if shuffle: random.shuffle(F)            \n",
    "        N = int(len(val)*perc+0.5)\n",
    "        xv += F[:N]\n",
    "        xt += F[N:]\n",
    "        \n",
    "    if shuffle:\n",
    "        random.shuffle(xt)\n",
    "        random.shuffle(xv)\n",
    "        \n",
    "    y_train = [label for _,label in xt]\n",
    "    x_train = [f for f,_ in xt]\n",
    "    y_valid = [label for _,label in xv]\n",
    "    x_valid = [f for f,_ in xv]\n",
    "    \n",
    "    return (x_train,y_train),(x_valid,y_valid)\n",
    "        \n",
    "    \n",
    "train, valid = split_data(df=min_loss_200_per_cat)\n",
    "!mkdir -p ./data_train\n",
    "!mkdir -p ./data_valid\n",
    "# create folders\n",
    "for c in set(train[1]): \n",
    "    os.makedirs(f'./min_losses_200_per_cat/{c}',exist_ok=True)\n",
    "    os.makedirs(f'./data_valid/{c}',exist_ok=True)\n",
    "for file,c in tqdm(zip(train[0],train[1])):\n",
    "    name = file.split('/')[-1]\n",
    "    shutil.copyfile(file,f'./min_losses_200_per_cat/{c}/{name}')\n",
    "for file,c in tqdm(zip(valid[0],valid[1])):\n",
    "    name = file.split('/')[-1]\n",
    "    shutil.copyfile(file,f'./data_valid/{c}/{name}')\n",
    "# !ls ./data_valid/bouzellouf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW, WE APPLY HERE OCR THAT WILL DETECT THE IMAGES WITH TEXT, WE REMOVE THE TEXT IMAGES FORM 1500 MIN LOSS IMAGES \n",
    "# We are removing the images that contains text from min_toplosses dataframe that contains 1000 images\n",
    "import pytesseract\n",
    "# import pandas as pd\n",
    "# from pathlib import Path\n",
    "# import os.path\n",
    "# from fastai.vision.all import *\n",
    "def optical_character_recognition(file, path = True):\n",
    "  \"\"\" Simple OCR of text from images.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  file: Path or str\n",
    "    Image to examine.\n",
    "  path: bool\n",
    "    Indicates whether the file passed in is a path to a file, or an already opened Pillow Image.\n",
    "    \n",
    "  Returns:\n",
    "  ----------\n",
    "  str\n",
    "    Text that was detected.\n",
    "  \"\"\"\n",
    "  if path == True:\n",
    "    # Use Pillow's Image class to open the image\n",
    "    img = Image.open(file)\n",
    "    new_size = tuple(2*x for x in img.size)\n",
    "    img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "    img = img.convert('L')\n",
    "    text = pytesseract.image_to_string(img, lang='eng', config='-c tessedit_char_whitelist=01234567890ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz --psm 3 --oem 3')\n",
    "    # Remove '\\n\\x0c' whiich is found in every image\n",
    "    text = text[:-3]\n",
    "  else: \n",
    "    # Already opened with Pillow\n",
    "    new_size = tuple(2*x for x in file.size)\n",
    "    img = file.resize(new_size, Image.Resampling.LANCZOS)\n",
    "    img = img.convert('L')\n",
    "    text = pytesseract.image_to_string(file, lang='eng', config='-c tessedit_char_whitelist=01234567890ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz --psm 3 --oem 3')\n",
    "    # Remove '\\n\\x0c' whiich is found in every image\n",
    "    text = text[:-3]\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_artificial_text(img_folder, edge_removal = True, verbose = False):\n",
    "  \"\"\" Searches for artificial text in each image in a image folder using pytesseract.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  img_folder: Path or str\n",
    "    Folder with images to examine.\n",
    "  edge_removal: bool\n",
    "    Should the function attempt to crop the edges of the to remove detected text.\n",
    "    If no text is found, the cropped image is saved.\n",
    "  verbose: bool\n",
    "    Should the function print out which images are cropped, and which images have been found to have text.  \n",
    "    \n",
    "  Returns:\n",
    "  ----------\n",
    "  DataFrame containing images marked as possessing artifical text. \n",
    "  \"\"\"\n",
    "  from tqdm.notebook import tqdm\n",
    "  # Initialize DataFrame\n",
    "  text_detected = []\n",
    "  classes = ['Cleaner-Wrasse',\n",
    " 'Broomtail-Wrasse',\n",
    " 'Clarks-anemonefish',\n",
    " 'Blackspot-Snapper',\n",
    " 'Yellowtail-Surgeonfish',\n",
    " 'Whaleshark',\n",
    " 'Blackfin-Butterflyfish',\n",
    " 'Ocellated-Eagle-Ray',\n",
    " 'Marbled_Electric_Ray',\n",
    " 'Longfin-Bannerfish']\n",
    "  column_names = ['Filepath', 'Text Found']\n",
    "  df_mt = pd.DataFrame(columns = column_names)\n",
    "  for image_path in tqdm(sorted(img_folder.ls())):\n",
    "    im = Image.open(image_path).convert('RGB')\n",
    "    if edge_removal == True:\n",
    "      # if the image has text\n",
    "      if len(optical_character_recognition(im, path = False))>0:\n",
    "        if verbose == True:print(f'Text detected in image {image_path.name}');\n",
    "        # if the text is along the top or bottom edge, overwrite the old image else make note of it in a dataframe\n",
    "        if len(optical_character_recognition((crop_image(image_path)).convert('RGB'), path = False)) == 0:\n",
    "          if verbose == True:print(f'Cropping removed text in image {image_path.name}');\n",
    "          crop_image(image_path).save(image_path)\n",
    "        else:\n",
    "          df_mt = df_mt.append({'Filepath':image_path,'Text Found':optical_character_recognition(image_path)}, ignore_index=True)\n",
    "    # No cropping      \n",
    "    else:\n",
    "      if len(optical_character_recognition(im, path=False))>0:\n",
    "        if verbose==True:print(f'Text detected in image {image_path.name}');\n",
    "        df_mt = df_mt.append({'Filepath':image_path,'Text Found':optical_character_recognition(image_path)}, ignore_index=True)\n",
    "\n",
    "  # Create dataframe containing images with artificial text towards the center\n",
    "  if verbose == True:print(f'{len(df_mt)} images with text found in folder {img_folder}.');\n",
    "  return df_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Cleaner-Wrasse',\n",
    " 'Broomtail-Wrasse',\n",
    " 'Clarks-anemonefish',\n",
    " 'Blackspot-Snapper',\n",
    " 'Yellowtail-Surgeonfish',\n",
    " 'Whaleshark',\n",
    " 'Blackfin-Butterflyfish',\n",
    " 'Ocellated-Eagle-Ray',\n",
    " 'Marbled_Electric_Ray',\n",
    " 'Longfin-Bannerfish']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look in a folder and grab a file\n",
    "df_artificial = []\n",
    "df = []\n",
    "local_path = Path('min_losses_200_per_cat')\n",
    "for i in range(len(classes)):\n",
    "    img_folder = local_path/classes[i]\n",
    "    df_artificial.append(find_artificial_text(img_folder, edge_removal = False, verbose = True))\n",
    "    df.append(df_artificial[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_detected_df1 = pd.concat(df_artificial)\n",
    "print(len(text_detected_df1))\n",
    "print(text_detected_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_txt_detected = text_detected_df1.to_csv('texted_images_from_min_loss_1500.csv', index = False)\n",
    "arabic_txt_detected = pd.read_csv('texted_images_from_min_loss_1500.csv')\n",
    "arabic_txt_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss_200_per_cat['new_path']=None\n",
    "min_loss_200_per_cat.reset_index(drop=True,inplace=True)\n",
    "min_loss_200_per_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li=str(min_loss_200_per_cat.iloc[0]['Filepath']).split('/')\n",
    "li[1]+'/'+li[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(min_loss_200_per_cat)):\n",
    "    li=str(min_loss_200_per_cat.iloc[i]['Filepath']).split('/')\n",
    "    min_loss_200_per_cat.at[i,'new_path']=li[1]+'/'+li[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_txt_detected['new_path']=None\n",
    "arabic_txt_detected.reset_index(drop=True,inplace=True)\n",
    "arabic_txt_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(arabic_txt_detected)):\n",
    "    li=str(arabic_txt_detected.iloc[i]['Filepath']).split('/')\n",
    "    arabic_txt_detected.at[i,'new_path']=li[1]+'/'+li[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_txt_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = min_loss_200_per_cat\n",
    "print(\"length of data is:\",len(min_loss['new_path']))\n",
    "txt_noisy_images = arabic_txt_detected\n",
    "print(\"length of text detected is:\",len(txt_noisy_images))\n",
    "txt_noisy_images_lst = list(txt_noisy_images['new_path'])\n",
    "# print(txt_noisy_images_lst)\n",
    "text_free = min_loss[~min_loss['new_path'].isin(txt_noisy_images_lst)]\n",
    "after_ocr = text_free.to_csv('text_free.csv', index = False)\n",
    "after_ocr = pd.read_csv('text_free.csv')\n",
    "after_ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 100 samples per class with min losses from all the data\n",
    "# import pandas as pd\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = after_ocr\n",
    "\n",
    "# Group the samples by category and select the 50 samples with the minimum loss\n",
    "per_cat_100 = df.loc[df.groupby('Label')['Loss'].nsmallest(100).index.get_level_values(1)]\n",
    "# condition = 0.00002  # change the condition to whatever value you want\n",
    "# fs_5_per_cat_after_ocr = df.loc[df.groupby('Label').apply(lambda x: x[x['los'] > condition].nsmallest(5, columns=['los'])).index.get_level_values(1)]\n",
    "\n",
    "\n",
    "# Save the selected DataFrame as a CSV file\n",
    "per_cat_100.to_csv('per_cat_100.csv', index=False)\n",
    "per_cat_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = per_cat_100\n",
    "\n",
    "# Determine the number of rows and columns needed to display all the images\n",
    "n = len(df)\n",
    "ncols = 5\n",
    "nrows = (n + ncols - 1) // ncols\n",
    "\n",
    "# Calculate the figure size based on the number of rows and columns\n",
    "figwidth = 20\n",
    "figheight = nrows * figwidth / ncols\n",
    "\n",
    "# Create a figure and subplot for each image\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(figwidth, figheight))\n",
    "\n",
    "# Plot each image\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    img = imread(row['Filepath'])\n",
    "    ax = axs[counter // ncols][counter % ncols]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Loss: {row['Loss']:.8f}\\nLabel: {row['Label']}\", color = 'red')\n",
    "    ax.axis('off')\n",
    "    counter += 1\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig('1000.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "\n",
    "# Helper method to split dataset into train and test folders\n",
    "def split_data(df:pd.core.frame.DataFrame=None,filepath=None,shuffle=True,perc=0.0):\n",
    "    def get_class(file):\n",
    "        return file.split('/')[-2].strip()\n",
    "    \n",
    "    if type(filepath)!=type(None):\n",
    "        files = [file for file in glob(filepath+'/**/*.jpg')]\n",
    "    else:\n",
    "        assert(type(df)!=type(None))\n",
    "        files = df['Filepath']\n",
    "        \n",
    "    classes = set([get_class(name) for name in files])\n",
    "    \n",
    "    D = { key : [] for key in classes }\n",
    "    for file in tqdm(files):\n",
    "        D[get_class(file)].append(file)\n",
    "    xt, xv = [],[]    \n",
    "    for key,val in D.items():\n",
    "        keys = [key for n in range(len(val))]\n",
    "        F = [(x,y) for x,y in zip(val,keys)]\n",
    "        if shuffle: random.shuffle(F)            \n",
    "        N = int(len(val)*perc+0.5)\n",
    "        xv += F[:N]\n",
    "        xt += F[N:]\n",
    "        \n",
    "    if shuffle:\n",
    "        random.shuffle(xt)\n",
    "        random.shuffle(xv)\n",
    "        \n",
    "    y_train = [label for _,label in xt]\n",
    "    x_train = [f for f,_ in xt]\n",
    "    y_valid = [label for _,label in xv]\n",
    "    x_valid = [f for f,_ in xv]\n",
    "    \n",
    "    return (x_train,y_train),(x_valid,y_valid)\n",
    "        \n",
    "    \n",
    "train, valid = split_data(df=per_cat_100)\n",
    "!mkdir -p ./data_train\n",
    "!mkdir -p ./data_valid\n",
    "# create folders\n",
    "for c in set(train[1]): \n",
    "    os.makedirs(f'./baseline_10_images_per_cat/{c}',exist_ok=True)\n",
    "    os.makedirs(f'./data_valid/{c}',exist_ok=True)\n",
    "for file,c in tqdm(zip(train[0],train[1])):\n",
    "    name = file.split('/')[-1]\n",
    "    shutil.copyfile(file,f'./baseline_10_images_per_cat/{c}/{name}')\n",
    "for file,c in tqdm(zip(valid[0],valid[1])):\n",
    "    name = file.split('/')[-1]\n",
    "    shutil.copyfile(file,f'./data_valid/{c}/{name}')\n",
    "# !ls ./data_valid/bouzellouf/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Select 50 per cat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 100 samples per class with min losses from all the data\n",
    "# import pandas as pd\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = per_cat_100\n",
    "\n",
    "# Group the samples by category and select the 50 samples with the minimum loss\n",
    "per_cat_50 = df.loc[df.groupby('Label')['Loss'].nsmallest(50).index.get_level_values(1)]\n",
    "# condition = 0.00002  # change the condition to whatever value you want\n",
    "# fs_5_per_cat_after_ocr = df.loc[df.groupby('Label').apply(lambda x: x[x['los'] > condition].nsmallest(5, columns=['los'])).index.get_level_values(1)]\n",
    "\n",
    "\n",
    "# Save the selected DataFrame as a CSV file\n",
    "per_cat_50.to_csv('per_cat_50.csv', index=False)\n",
    "per_cat_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Display 50 images per cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = per_cat_50\n",
    "\n",
    "# Determine the number of rows and columns needed to display all the images\n",
    "n = len(df)\n",
    "ncols = 5\n",
    "nrows = (n + ncols - 1) // ncols\n",
    "\n",
    "# Calculate the figure size based on the number of rows and columns\n",
    "figwidth = 20\n",
    "figheight = nrows * figwidth / ncols\n",
    "\n",
    "# Create a figure and subplot for each image\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(figwidth, figheight))\n",
    "\n",
    "# Plot each image\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    img = imread(row['Filepath'])\n",
    "    ax = axs[counter // ncols][counter % ncols]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Loss: {row['Loss']:.8f}\\nLabel: {row['Label']}\", color = 'red')\n",
    "    ax.axis('off')\n",
    "    counter += 1\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig('50.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# copy images to folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "\n",
    "# Helper method to split dataset into train and test folders\n",
    "def split_data(df:pd.core.frame.DataFrame=None,filepath=None,shuffle=True,perc=0.0):\n",
    "    def get_class(file):\n",
    "        return file.split('/')[-2].strip()\n",
    "    \n",
    "    if type(filepath)!=type(None):\n",
    "        files = [file for file in glob(filepath+'/**/*.jpg')]\n",
    "    else:\n",
    "        assert(type(df)!=type(None))\n",
    "        files = df['Filepath']\n",
    "        \n",
    "    classes = set([get_class(name) for name in files])\n",
    "    \n",
    "    D = { key : [] for key in classes }\n",
    "    for file in tqdm(files):\n",
    "        D[get_class(file)].append(file)\n",
    "    xt, xv = [],[]    \n",
    "    for key,val in D.items():\n",
    "        keys = [key for n in range(len(val))]\n",
    "        F = [(x,y) for x,y in zip(val,keys)]\n",
    "        if shuffle: random.shuffle(F)            \n",
    "        N = int(len(val)*perc+0.5)\n",
    "        xv += F[:N]\n",
    "        xt += F[N:]\n",
    "        \n",
    "    if shuffle:\n",
    "        random.shuffle(xt)\n",
    "        random.shuffle(xv)\n",
    "        \n",
    "    y_train = [label for _,label in xt]\n",
    "    x_train = [f for f,_ in xt]\n",
    "    y_valid = [label for _,label in xv]\n",
    "    x_valid = [f for f,_ in xv]\n",
    "    \n",
    "    return (x_train,y_train),(x_valid,y_valid)\n",
    "        \n",
    "    \n",
    "train, valid = split_data(df=per_cat_5_inverse)\n",
    "!mkdir -p ./data_train\n",
    "!mkdir -p ./data_valid\n",
    "# create folders\n",
    "for c in set(train[1]): \n",
    "    os.makedirs(f'./per_cat_50/{c}',exist_ok=True)\n",
    "    os.makedirs(f'./data_valid/{c}',exist_ok=True)\n",
    "for file,c in tqdm(zip(train[0],train[1])):\n",
    "    name = file.split('/')[-1]\n",
    "    shutil.copyfile(file,f'./per_cat_50/{c}/{name}')\n",
    "for file,c in tqdm(zip(valid[0],valid[1])):\n",
    "    name = file.split('/')[-1]\n",
    "    shutil.copyfile(file,f'./data_valid/{c}/{name}')\n",
    "# !ls ./data_valid/bouzellouf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After select 50 images per cat with min losses- update the csv then go for 20 per cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_cat_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = per_cat_100\n",
    "aa\n",
    "bb = per_cat_50\n",
    "cc = list(bb['Filepath'])\n",
    "updated_data = aa[~aa['Filepath'].isin(cc)]\n",
    "updated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Select 20 images per cat with min losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 100 samples per class with min losses from all the data\n",
    "# import pandas as pd\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = updated_data\n",
    "\n",
    "# Group the samples by category and select the 50 samples with the minimum loss\n",
    "per_cat_20 = df.loc[df.groupby('Label')['Loss'].nlargest(20).index.get_level_values(1)]\n",
    "# condition = 0.00002  # change the condition to whatever value you want\n",
    "# fs_20_per_cat_after_ocr = df.loc[df.groupby('Label').apply(lambda x: x[x['Loss'] > condition].nsmallest(5, columns=['Loss'])).index.get_level_values(1)]\n",
    "\n",
    "\n",
    "# Save the selected DataFrame as a CSV file\n",
    "per_cat_20.to_csv('per_cat_20.csv', index=False)\n",
    "per_cat_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display 20_cat per cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = per_cat_20\n",
    "\n",
    "# Determine the number of rows and columns needed to display all the images\n",
    "n = len(df)\n",
    "ncols = 5\n",
    "nrows = (n + ncols - 1) // ncols\n",
    "\n",
    "# Calculate the figure size based on the number of rows and columns\n",
    "figwidth = 20\n",
    "figheight = nrows * figwidth / ncols\n",
    "\n",
    "# Create a figure and subplot for each image\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(figwidth, figheight))\n",
    "\n",
    "# Plot each image\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    img = imread(row['Filepath'])\n",
    "    ax = axs[counter // ncols][counter % ncols]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Loss: {row['Loss']:.8f}\\nLabel: {row['Label']}\", color = 'red')\n",
    "    ax.axis('off')\n",
    "    counter += 1\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig('20.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "\n",
    "# Helper method to split dataset into train and test folders\n",
    "def split_data(df:pd.core.frame.DataFrame=None,filepath=None,shuffle=True,perc=0.0):\n",
    "    def get_class(file):\n",
    "        return file.split('/')[-2].strip()\n",
    "    \n",
    "    if type(filepath)!=type(None):\n",
    "        files = [file for file in glob(filepath+'/**/*.jpg')]\n",
    "    else:\n",
    "        assert(type(df)!=type(None))\n",
    "        files = df['Filepath']\n",
    "        \n",
    "    classes = set([get_class(name) for name in files])\n",
    "    \n",
    "    D = { key : [] for key in classes }\n",
    "    for file in tqdm(files):\n",
    "        D[get_class(file)].append(file)\n",
    "    xt, xv = [],[]    \n",
    "    for key,val in D.items():\n",
    "        keys = [key for n in range(len(val))]\n",
    "        F = [(x,y) for x,y in zip(val,keys)]\n",
    "        if shuffle: random.shuffle(F)            \n",
    "        N = int(len(val)*perc+0.5)\n",
    "        xv += F[:N]\n",
    "        xt += F[N:]\n",
    "        \n",
    "    if shuffle:\n",
    "        random.shuffle(xt)\n",
    "        random.shuffle(xv)\n",
    "        \n",
    "    y_train = [label for _,label in xt]\n",
    "    x_train = [f for f,_ in xt]\n",
    "    y_valid = [label for _,label in xv]\n",
    "    x_valid = [f for f,_ in xv]\n",
    "    \n",
    "    return (x_train,y_train),(x_valid,y_valid)\n",
    "        \n",
    "    \n",
    "train, valid = split_data(df=per_cat_20)\n",
    "!mkdir -p ./data_train\n",
    "!mkdir -p ./data_valid\n",
    "# create folders\n",
    "for c in set(train[1]): \n",
    "    os.makedirs(f'./per_cat_20/{c}',exist_ok=True)\n",
    "    os.makedirs(f'./data_valid/{c}',exist_ok=True)\n",
    "for file,c in tqdm(zip(train[0],train[1])):\n",
    "    name = file.split('/')[-1]\n",
    "    shutil.copyfile(file,f'./per_cat_20/{c}/{name}')\n",
    "for file,c in tqdm(zip(valid[0],valid[1])):\n",
    "    name = file.split('/')[-1]\n",
    "    shutil.copyfile(file,f'./data_valid/{c}/{name}')\n",
    "# !ls ./data_valid/bouzellouf/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Select 20 per cat update csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = updated_data\n",
    "aa\n",
    "bb = per_cat_20\n",
    "cc = list(bb['Filepath'])\n",
    "updated_data = aa[~aa['Filepath'].isin(cc)]\n",
    "updated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Select 10 images per cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 100 samples per class with min losses from all the data\n",
    "# import pandas as pd\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = updated_data\n",
    "\n",
    "# Group the samples by category and select the 50 samples with the minimum loss\n",
    "per_cat_10 = df.loc[df.groupby('Label')['Loss'].nsmallest(10).index.get_level_values(1)]\n",
    "# condition = 0.00002  # change the condition to whatever value you want\n",
    "# fs_5_per_cat_after_ocr = df.loc[df.groupby('Label').apply(lambda x: x[x['los'] > condition].nsmallest(5, columns=['los'])).index.get_level_values(1)]\n",
    "\n",
    "\n",
    "# Save the selected DataFrame as a CSV file\n",
    "per_cat_10.to_csv('per_cat_10.csv', index=False)\n",
    "per_cat_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = per_cat_10\n",
    "\n",
    "# Determine the number of rows and columns needed to display all the images\n",
    "n = len(df)\n",
    "ncols = 5\n",
    "nrows = (n + ncols - 1) // ncols\n",
    "\n",
    "# Calculate the figure size based on the number of rows and columns\n",
    "figwidth = 20\n",
    "figheight = nrows * figwidth / ncols\n",
    "\n",
    "# Create a figure and subplot for each image\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(figwidth, figheight))\n",
    "\n",
    "# Plot each image\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    img = imread(row['Filepath'])\n",
    "    ax = axs[counter // ncols][counter % ncols]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Loss: {row['Loss']:.8f}\\nLabel: {row['Label']}\", color = 'red')\n",
    "    ax.axis('off')\n",
    "    counter += 1\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig('10.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "\n",
    "# Helper method to split dataset into train and test folders\n",
    "def split_data(df:pd.core.frame.DataFrame=None,filepath=None,shuffle=True,perc=0.0):\n",
    "    def get_class(file):\n",
    "        return file.split('/')[-2].strip()\n",
    "    \n",
    "    if type(filepath)!=type(None):\n",
    "        files = [file for file in glob(filepath+'/**/*.jpg')]\n",
    "    else:\n",
    "        assert(type(df)!=type(None))\n",
    "        files = df['Filepath']\n",
    "        \n",
    "    classes = set([get_class(name) for name in files])\n",
    "    \n",
    "    D = { key : [] for key in classes }\n",
    "    for file in tqdm(files):\n",
    "        D[get_class(file)].append(file)\n",
    "    xt, xv = [],[]    \n",
    "    for key,val in D.items():\n",
    "        keys = [key for n in range(len(val))]\n",
    "        F = [(x,y) for x,y in zip(val,keys)]\n",
    "        if shuffle: random.shuffle(F)            \n",
    "        N = int(len(val)*perc+0.5)\n",
    "        xv += F[:N]\n",
    "        xt += F[N:]\n",
    "        \n",
    "    if shuffle:\n",
    "        random.shuffle(xt)\n",
    "        random.shuffle(xv)\n",
    "        \n",
    "    y_train = [label for _,label in xt]\n",
    "    x_train = [f for f,_ in xt]\n",
    "    y_valid = [label for _,label in xv]\n",
    "    x_valid = [f for f,_ in xv]\n",
    "    \n",
    "    return (x_train,y_train),(x_valid,y_valid)\n",
    "        \n",
    "    \n",
    "train, valid = split_data(df=per_cat_10)\n",
    "!mkdir -p ./data_train\n",
    "!mkdir -p ./data_valid\n",
    "# create folders\n",
    "for c in set(train[1]): \n",
    "    os.makedirs(f'./per_cat_10/{c}',exist_ok=True)\n",
    "    os.makedirs(f'./data_valid/{c}',exist_ok=True)\n",
    "for file,c in tqdm(zip(train[0],train[1])):\n",
    "    name = file.split('/')[-1]\n",
    "    shutil.copyfile(file,f'./per_cat_10/{c}/{name}')\n",
    "for file,c in tqdm(zip(valid[0],valid[1])):\n",
    "    name = file.split('/')[-1]\n",
    "    shutil.copyfile(file,f'./data_valid/{c}/{name}')\n",
    "# !ls ./data_valid/bouzellouf/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update after selecting 10 images per cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = updated_data\n",
    "aa\n",
    "bb = per_cat_10\n",
    "cc = list(bb['Filepath'])\n",
    "updated_data = aa[~aa['Filepath'].isin(cc)]\n",
    "updated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-select 5 images per cat with min losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 100 samples per class with min losses from all the data\n",
    "# import pandas as pd\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = updated_data\n",
    "\n",
    "# Group the samples by category and select the 50 samples with the minimum loss\n",
    "per_cat_5 = df.loc[df.groupby('Label')['Loss'].nsmallest(5).index.get_level_values(1)]\n",
    "# condition = 0.00002  # change the condition to whatever value you want\n",
    "# fs_5_per_cat_after_ocr = df.loc[df.groupby('Label').apply(lambda x: x[x['los'] > condition].nsmallest(5, columns=['los'])).index.get_level_values(1)]\n",
    "\n",
    "\n",
    "# Save the selected DataFrame as a CSV file\n",
    "per_cat_5.to_csv('per_cat_5.csv', index=False)\n",
    "per_cat_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display 5 per cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = per_cat_5\n",
    "\n",
    "# Determine the number of rows and columns needed to display all the images\n",
    "n = len(df)\n",
    "ncols = 5\n",
    "nrows = (n + ncols - 1) // ncols\n",
    "\n",
    "# Calculate the figure size based on the number of rows and columns\n",
    "figwidth = 20\n",
    "figheight = nrows * figwidth / ncols\n",
    "\n",
    "# Create a figure and subplot for each image\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(figwidth, figheight))\n",
    "\n",
    "# Plot each image\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    img = imread(row['Filepath'])\n",
    "    ax = axs[counter // ncols][counter % ncols]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Loss: {row['Loss']:.8f}\\nLabel: {row['Label']}\", color = 'red')\n",
    "    ax.axis('off')\n",
    "    counter += 1\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig('5.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "\n",
    "# Helper method to split dataset into train and test folders\n",
    "def split_data(df:pd.core.frame.DataFrame=None,filepath=None,shuffle=True,perc=0.0):\n",
    "    def get_class(file):\n",
    "        return file.split('/')[-2].strip()\n",
    "    \n",
    "    if type(filepath)!=type(None):\n",
    "        files = [file for file in glob(filepath+'/**/*.jpg')]\n",
    "    else:\n",
    "        assert(type(df)!=type(None))\n",
    "        files = df['Filepath']\n",
    "        \n",
    "    classes = set([get_class(name) for name in files])\n",
    "    \n",
    "    D = { key : [] for key in classes }\n",
    "    for file in tqdm(files):\n",
    "        D[get_class(file)].append(file)\n",
    "    xt, xv = [],[]    \n",
    "    for key,val in D.items():\n",
    "        keys = [key for n in range(len(val))]\n",
    "        F = [(x,y) for x,y in zip(val,keys)]\n",
    "        if shuffle: random.shuffle(F)            \n",
    "        N = int(len(val)*perc+0.5)\n",
    "        xv += F[:N]\n",
    "        xt += F[N:]\n",
    "        \n",
    "    if shuffle:\n",
    "        random.shuffle(xt)\n",
    "        random.shuffle(xv)\n",
    "        \n",
    "    y_train = [label for _,label in xt]\n",
    "    x_train = [f for f,_ in xt]\n",
    "    y_valid = [label for _,label in xv]\n",
    "    x_valid = [f for f,_ in xv]\n",
    "    \n",
    "    return (x_train,y_train),(x_valid,y_valid)\n",
    "        \n",
    "    \n",
    "train, valid = split_data(df=per_cat_5)\n",
    "!mkdir -p ./data_train\n",
    "!mkdir -p ./data_valid\n",
    "# create folders\n",
    "for c in set(train[1]): \n",
    "    os.makedirs(f'./per_cat_5/{c}',exist_ok=True)\n",
    "    os.makedirs(f'./data_valid/{c}',exist_ok=True)\n",
    "for file,c in tqdm(zip(train[0],train[1])):\n",
    "    name = file.split('/')[-1]\n",
    "    shutil.copyfile(file,f'./per_cat_5/{c}/{name}')\n",
    "for file,c in tqdm(zip(valid[0],valid[1])):\n",
    "    name = file.split('/')[-1]\n",
    "    shutil.copyfile(file,f'./data_valid/{c}/{name}')\n",
    "# !ls ./data_valid/bouzellouf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = updated_data\n",
    "aa\n",
    "bb = per_cat_5\n",
    "cc = list(bb['Filepath'])\n",
    "updated_data = aa[~aa['Filepath'].isin(cc)]\n",
    "updated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_cat_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 100 samples per class with min losses from all the data\n",
    "# import pandas as pd\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = per_cat_50\n",
    "\n",
    "# Group the samples by category and select the 50 samples with the minimum loss\n",
    "per_cat_10_1 = df.loc[df.groupby('Label')['Loss'].nsmallest(10).index.get_level_values(1)]\n",
    "# condition = 0.00002  # change the condition to whatever value you want\n",
    "# fs_5_per_cat_after_ocr = df.loc[df.groupby('Label').apply(lambda x: x[x['los'] > condition].nsmallest(5, columns=['los'])).index.get_level_values(1)]\n",
    "\n",
    "\n",
    "# Save the selected DataFrame as a CSV file\n",
    "per_cat_10_1.to_csv('per_cat_10_1.csv', index=False)\n",
    "per_cat_10_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = per_cat_10_1\n",
    "\n",
    "# Determine the number of rows and columns needed to display all the images\n",
    "n = len(df)\n",
    "ncols = 5\n",
    "nrows = (n + ncols - 1) // ncols\n",
    "\n",
    "# Calculate the figure size based on the number of rows and columns\n",
    "figwidth = 20\n",
    "figheight = nrows * figwidth / ncols\n",
    "\n",
    "# Create a figure and subplot for each image\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(figwidth, figheight))\n",
    "\n",
    "# Plot each image\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    img = imread(row['Filepath'])\n",
    "    ax = axs[counter // ncols][counter % ncols]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Loss: {row['Loss']:.8f}\\nLabel: {row['Label']}\", color = 'red')\n",
    "    ax.axis('off')\n",
    "    counter += 1\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig('per_cat_10_1.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "\n",
    "# Helper method to split dataset into train and test folders\n",
    "def split_data(df:pd.core.frame.DataFrame=None,filepath=None,shuffle=True,perc=0.0):\n",
    "    def get_class(file):\n",
    "        return file.split('/')[-2].strip()\n",
    "    \n",
    "    if type(filepath)!=type(None):\n",
    "        files = [file for file in glob(filepath+'/**/*.jpg')]\n",
    "    else:\n",
    "        assert(type(df)!=type(None))\n",
    "        files = df['Filepath']\n",
    "        \n",
    "    classes = set([get_class(name) for name in files])\n",
    "    \n",
    "    D = { key : [] for key in classes }\n",
    "    for file in tqdm(files):\n",
    "        D[get_class(file)].append(file)\n",
    "    xt, xv = [],[]    \n",
    "    for key,val in D.items():\n",
    "        keys = [key for n in range(len(val))]\n",
    "        F = [(x,y) for x,y in zip(val,keys)]\n",
    "        if shuffle: random.shuffle(F)            \n",
    "        N = int(len(val)*perc+0.5)\n",
    "        xv += F[:N]\n",
    "        xt += F[N:]\n",
    "        \n",
    "    if shuffle:\n",
    "        random.shuffle(xt)\n",
    "        random.shuffle(xv)\n",
    "        \n",
    "    y_train = [label for _,label in xt]\n",
    "    x_train = [f for f,_ in xt]\n",
    "    y_valid = [label for _,label in xv]\n",
    "    x_valid = [f for f,_ in xv]\n",
    "    \n",
    "    return (x_train,y_train),(x_valid,y_valid)\n",
    "        \n",
    "    \n",
    "train, valid = split_data(df=per_cat_10_1)\n",
    "!mkdir -p ./data_train\n",
    "!mkdir -p ./data_valid\n",
    "# create folders\n",
    "for c in set(train[1]): \n",
    "    os.makedirs(f'./per_cat_10_1/{c}',exist_ok=True)\n",
    "    os.makedirs(f'./data_valid/{c}',exist_ok=True)\n",
    "for file,c in tqdm(zip(train[0],train[1])):\n",
    "    name = file.split('/')[-1]\n",
    "    shutil.copyfile(file,f'./per_cat_10_1/{c}/{name}')\n",
    "for file,c in tqdm(zip(valid[0],valid[1])):\n",
    "    name = file.split('/')[-1]\n",
    "    shutil.copyfile(file,f'./data_valid/{c}/{name}')\n",
    "# !ls ./data_valid/bouzellouf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = per_cat_50\n",
    "aa\n",
    "bb = per_cat_10_1\n",
    "cc = list(bb['Filepath'])\n",
    "updated_data = aa[~aa['Filepath'].isin(cc)]\n",
    "updated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 100 samples per class with min losses from all the data\n",
    "# import pandas as pd\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = updated_data\n",
    "\n",
    "# Group the samples by category and select the 50 samples with the minimum loss\n",
    "per_cat_5_2 = df.loc[df.groupby('Label')['Loss'].nsmallest(5).index.get_level_values(1)]\n",
    "# condition = 0.00002  # change the condition to whatever value you want\n",
    "# fs_5_per_cat_after_ocr = df.loc[df.groupby('Label').apply(lambda x: x[x['los'] > condition].nsmallest(5, columns=['los'])).index.get_level_values(1)]\n",
    "\n",
    "\n",
    "# Save the selected DataFrame as a CSV file\n",
    "per_cat_5_2.to_csv('per_cat_5_2.csv', index=False)\n",
    "per_cat_5_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = per_cat_5_2\n",
    "\n",
    "# Determine the number of rows and columns needed to display all the images\n",
    "n = len(df)\n",
    "ncols = 5\n",
    "nrows = (n + ncols - 1) // ncols\n",
    "\n",
    "# Calculate the figure size based on the number of rows and columns\n",
    "figwidth = 20\n",
    "figheight = nrows * figwidth / ncols\n",
    "\n",
    "# Create a figure and subplot for each image\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(figwidth, figheight))\n",
    "\n",
    "# Plot each image\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    img = imread(row['Filepath'])\n",
    "    ax = axs[counter // ncols][counter % ncols]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Loss: {row['Loss']:.8f}\\nLabel: {row['Label']}\", color = 'red')\n",
    "    ax.axis('off')\n",
    "    counter += 1\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig('5_2.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "\n",
    "# Helper method to split dataset into train and test folders\n",
    "def split_data(df:pd.core.frame.DataFrame=None,filepath=None,shuffle=True,perc=0.0):\n",
    "    def get_class(file):\n",
    "        return file.split('/')[-2].strip()\n",
    "    \n",
    "    if type(filepath)!=type(None):\n",
    "        files = [file for file in glob(filepath+'/**/*.jpg')]\n",
    "    else:\n",
    "        assert(type(df)!=type(None))\n",
    "        files = df['Filepath']\n",
    "        \n",
    "    classes = set([get_class(name) for name in files])\n",
    "    \n",
    "    D = { key : [] for key in classes }\n",
    "    for file in tqdm(files):\n",
    "        D[get_class(file)].append(file)\n",
    "    xt, xv = [],[]    \n",
    "    for key,val in D.items():\n",
    "        keys = [key for n in range(len(val))]\n",
    "        F = [(x,y) for x,y in zip(val,keys)]\n",
    "        if shuffle: random.shuffle(F)            \n",
    "        N = int(len(val)*perc+0.5)\n",
    "        xv += F[:N]\n",
    "        xt += F[N:]\n",
    "        \n",
    "    if shuffle:\n",
    "        random.shuffle(xt)\n",
    "        random.shuffle(xv)\n",
    "        \n",
    "    y_train = [label for _,label in xt]\n",
    "    x_train = [f for f,_ in xt]\n",
    "    y_valid = [label for _,label in xv]\n",
    "    x_valid = [f for f,_ in xv]\n",
    "    \n",
    "    return (x_train,y_train),(x_valid,y_valid)\n",
    "        \n",
    "    \n",
    "train, valid = split_data(df=per_cat_5_1)\n",
    "!mkdir -p ./data_train\n",
    "!mkdir -p ./data_valid\n",
    "# create folders\n",
    "for c in set(train[1]): \n",
    "    os.makedirs(f'./per_cat_5_2/{c}',exist_ok=True)\n",
    "    os.makedirs(f'./data_valid/{c}',exist_ok=True)\n",
    "for file,c in tqdm(zip(train[0],train[1])):\n",
    "    name = file.split('/')[-1]\n",
    "    shutil.copyfile(file,f'./per_cat_5_2/{c}/{name}')\n",
    "for file,c in tqdm(zip(valid[0],valid[1])):\n",
    "    name = file.split('/')[-1]\n",
    "    shutil.copyfile(file,f'./data_valid/{c}/{name}')\n",
    "# !ls ./data_valid/bouzellouf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = updated_data\n",
    "aa\n",
    "bb = per_cat_5_3\n",
    "cc = list(bb['Filepath'])\n",
    "updated_data = aa[~aa['Filepath'].isin(cc)]\n",
    "updated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 100 samples per class with min losses from all the data\n",
    "# import pandas as pd\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = updated_data\n",
    "\n",
    "# Group the samples by category and select the 50 samples with the minimum loss\n",
    "per_cat_5_3 = df.loc[df.groupby('Label')['Loss'].nsmallest(5).index.get_level_values(1)]\n",
    "# condition = 0.00002  # change the condition to whatever value you want\n",
    "# fs_5_per_cat_after_ocr = df.loc[df.groupby('Label').apply(lambda x: x[x['los'] > condition].nsmallest(5, columns=['los'])).index.get_level_values(1)]\n",
    "\n",
    "\n",
    "# Save the selected DataFrame as a CSV file\n",
    "per_cat_5_3.to_csv('per_cat_5_3.csv', index=False)\n",
    "per_cat_5_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = per_cat_5_3\n",
    "\n",
    "# Determine the number of rows and columns needed to display all the images\n",
    "n = len(df)\n",
    "ncols = 5\n",
    "nrows = (n + ncols - 1) // ncols\n",
    "\n",
    "# Calculate the figure size based on the number of rows and columns\n",
    "figwidth = 20\n",
    "figheight = nrows * figwidth / ncols\n",
    "\n",
    "# Create a figure and subplot for each image\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(figwidth, figheight))\n",
    "\n",
    "# Plot each image\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    img = imread(row['Filepath'])\n",
    "    ax = axs[counter // ncols][counter % ncols]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Loss: {row['Loss']:.8f}\\nLabel: {row['Label']}\", color = 'red')\n",
    "    ax.axis('off')\n",
    "    counter += 1\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig('5_3.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_cat_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_rows = per_cat_50[per_cat_50['Label'] == 'Blackfin-Butterflyfish']\n",
    "cat_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "# Load the DataFrame containing filepath, label, and loss value\n",
    "df = cat_rows\n",
    "\n",
    "# Determine the number of rows and columns needed to display all the images\n",
    "n = len(df)\n",
    "ncols = 5\n",
    "nrows = (n + ncols - 1) // ncols\n",
    "\n",
    "# Calculate the figure size based on the number of rows and columns\n",
    "figwidth = 20\n",
    "figheight = nrows * figwidth / ncols\n",
    "\n",
    "# Create a figure and subplot for each image\n",
    "fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=(figwidth, figheight))\n",
    "\n",
    "# Plot each image\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    img = imread(row['Filepath'])\n",
    "    ax = axs[counter // ncols][counter % ncols]\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Loss: {row['Loss']:.8f}\\nLabel: {row['Label']}\", color = 'red')\n",
    "    ax.axis('off')\n",
    "    counter += 1\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig('butterflyfish.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Food101_Kaggle.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
