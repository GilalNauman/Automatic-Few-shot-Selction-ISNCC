{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "41a6777d5e67dc652f57ce9681b4c44dc44152be",
    "colab_type": "text",
    "id": "uNaVQGQ9tQRr"
   },
   "source": [
    "<h1><center><b>Tropical Fishes</b></center></h1>\n",
    "<h1><center><b>Scrapped Data with 10 classes-CE Loss Function</b></center></h1>\n",
    "<center><b>15 March 2023</b></center><br/>\n",
    "<center><b>ISNCC IEEE @HBKU</b></center><br/>\n",
    "<center><b>Nauman Ullah Gilal</b></center><br/>\n",
    "<center><b>Co-Authors: Fahad Majeed, Khaled Al-Thelaya, Mehak Khan, Jens Schneider, and Marco Agus</b></center><br/>\n",
    "<center><b>IDEAL Lab</b></center><br/>\n",
    "<center><b>College of Science and Engineering</b></center><br/>\n",
    "<center><b>Hamad Bin Khalifa University<b></center><br/></h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "Check training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls scrapped_dataset_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:24.694292Z",
     "start_time": "2021-11-26T12:31:24.526520Z"
    },
    "_uuid": "7ceb7f04d55f3d1510a03868713e773e7df17046",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "7wJ_OH1DQyrd",
    "outputId": "bda2768c-24b9-4962-b7eb-fb3b07d6fad4"
   },
   "outputs": [],
   "source": [
    "!ls test\n",
    "\n",
    "# train_path = '/workspace/Nauman/MENA/MENA-v1-4C-Gym-Scheme/MENA-v1-c-109/MENA-v1-c-109/Training'\n",
    "# test_path = '/workspace/Nauman/MENA/MENA-v1-4C-Gym-Scheme/MENA-v1-c-109/MENA-v1-c-109/Testing'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set Categories, Classes, Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:24.701524Z",
     "start_time": "2021-11-26T12:31:24.696013Z"
    },
    "_uuid": "9a3e091f8d5db4be5a553a2fd23970dde7c649f2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1855
    },
    "colab_type": "code",
    "id": "yy_pAK35Rbdi",
    "outputId": "9b374f07-961a-4878-85a2-86589b2f68cb",
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "categories = os.listdir('scrapped_dataset_new')\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:24.704167Z",
     "start_time": "2021-11-26T12:31:24.702349Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"scrapped_dataset_new\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:25.951403Z",
     "start_time": "2021-11-26T12:31:24.704976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the training data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "rows = 2\n",
    "cols = 5\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(60,60))\n",
    "fig.suptitle(\"Showing one random image from each training class\", y=1.05, fontsize=40, weight = 'bold') # Adding  y=1.05, fontsize=24 helped me fix the suptitle overlapping with axes issue\n",
    "data_dir = TRAIN_DIR\n",
    "foods_sorted = sorted(os.listdir(data_dir))\n",
    "food_id = 0\n",
    "for i in range(rows):\n",
    "  for j in range(cols):\n",
    "    try:\n",
    "      food_selected = foods_sorted[food_id] \n",
    "      food_id += 1\n",
    "    except:\n",
    "      break\n",
    "    if food_selected == '.DS_Store':\n",
    "        continue\n",
    "    food_selected_images = os.listdir(os.path.join(data_dir,food_selected)) # returns the list of all files present in each food category\n",
    "    food_selected_random = np.random.choice(food_selected_images) # picks one food item from the list as choice, takes a list and returns one random item\n",
    "    img = plt.imread(os.path.join(data_dir,food_selected, food_selected_random))\n",
    "    ax[i][j].imshow(img)\n",
    "    ax[i][j].set_title(food_selected, pad = 10,weight = 'bold', color = 'red', fontsize = 30)\n",
    "    \n",
    "plt.setp(ax, xticks=[],yticks=[])\n",
    "plt.tight_layout()\n",
    "# https://matplotlib.org/users/tight_layout_guide.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of samples from Test set 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "rows = 2\n",
    "cols = 5\n",
    "fig, ax = plt.subplots(rows, cols, figsize=(60,60))\n",
    "fig.suptitle(\"Showing one random image from each training class\", y=1.05, fontsize=40, weight = 'bold') # Adding  y=1.05, fontsize=24 helped me fix the suptitle overlapping with axes issue\n",
    "data_dir = 'test'\n",
    "foods_sorted = sorted(os.listdir(data_dir))\n",
    "food_id = 0\n",
    "for i in range(rows):\n",
    "  for j in range(cols):\n",
    "    try:\n",
    "      food_selected = foods_sorted[food_id] \n",
    "      food_id += 1\n",
    "    except:\n",
    "      break\n",
    "    if food_selected == '.DS_Store':\n",
    "        continue\n",
    "    food_selected_images = os.listdir(os.path.join(data_dir,food_selected)) # returns the list of all files present in each food category\n",
    "    food_selected_random = np.random.choice(food_selected_images) # picks one food item from the list as choice, takes a list and returns one random item\n",
    "    img = plt.imread(os.path.join(data_dir,food_selected, food_selected_random))\n",
    "    ax[i][j].imshow(img)\n",
    "    ax[i][j].set_title(food_selected, pad = 10,weight = 'bold', color = 'green', fontsize = 30)\n",
    "    \n",
    "plt.setp(ax, xticks=[],yticks=[])\n",
    "plt.tight_layout()\n",
    "# https://matplotlib.org/users/tight_layout_guide.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaration of Efficient Net family with Resolution, Epochs, Learning Rate, Batch size and Label smoothing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.235114Z",
     "start_time": "2021-11-26T12:31:28.215051Z"
    }
   },
   "outputs": [],
   "source": [
    "architecture = 'efficientnet-b0'\n",
    "logfile = 'efficientnet-b0-MENA.csv'\n",
    "\n",
    "## The tables\n",
    "effnet_size = ({\n",
    "    'efficientnet-b0':300,      # original resolution\n",
    "    'efficientnet-b1':240,      # original resolution\n",
    "    'efficientnet-b2':400,      # original resolution\n",
    "    'efficientnet-b3':400,      # original resolution\n",
    "    'efficientnet-b4':500,      # original resolution\n",
    "    'efficientnet-b5':456,      # original resolution\n",
    "    'efficientnet-b6':528,      # original resolution\n",
    "    'efficientnet-b7':600,      # original resolution\n",
    "    'efficientnet-lite0':224,\n",
    "    'efficientnet-lite2':260,\n",
    "    'efficientnet-lite4':384,\n",
    "    'resnet18':224,\n",
    "    'resnet34':224,\n",
    "    'resnet50':224,\n",
    "    'resnet101':224,\n",
    "    'resnet152':224\n",
    "})\n",
    "batch_size = ({ \n",
    "    'efficientnet-b0':128,      # initially 160\n",
    "    'efficientnet-b1':128,      #\n",
    "    'efficientnet-b2':128,       #88 160\n",
    "    'efficientnet-b3':16,       #\n",
    "    'efficientnet-b4':16,       #\n",
    "    'efficientnet-b5':14,       #\n",
    "    'efficientnet-b6':16,\n",
    "    'efficientnet-b7':8,\n",
    "    'efficientnet-lite0': 160,\n",
    "    'efficientnet-lite2': 160,\n",
    "    'efficientnet-lite4':24,\n",
    "    \n",
    "    'resnet18':400,             # 3080: ok\n",
    "    'resnet34':320,            # RTX 3090 rocz!\n",
    "    'resnet50':256,             # 3080: ok\n",
    "    'resnet101':96,\n",
    "    'resnet152':64\n",
    "})\n",
    "learning_rate = ({\n",
    "    'efficientnet-b0':5e-4,     # 2e-3: too large, 5e-4: too large\n",
    "    'efficientnet-b1':2e-3,     # 1e-3: 82.53/93.9\n",
    "    'efficientnet-b2':1e-3,     # 1e-3, 2e-3     \n",
    "    'efficientnet-b3':2e-3,    \n",
    "    'efficientnet-b4':2e-3,  \n",
    "    'efficientnet-b5':1.5e-3,  \n",
    "    'efficientnet-b6':4e-3,\n",
    "    'efficientnet-b7':4e-3,\n",
    "    'efficientnet-lite0': 5e-4,\n",
    "    'efficientnet-lite2':1e-3,\n",
    "    'efficientnet-lite4':5e-4,\n",
    "    'resnet18':2.823e-02,\n",
    "    'resnet34':-1,\n",
    "    'resnet50':1.016e-02,\n",
    "    'resnet101':-1,\n",
    "    'resnet152':-1,\n",
    "})\n",
    "epochs = ({\n",
    "    'efficientnet-b0':(1,100), # for initally top losses\n",
    "    'efficientnet-b1':(6,20),\n",
    "    'efficientnet-b2':(15,50),\n",
    "    'efficientnet-b4':(5,25),\n",
    "    'efficientnet-lite0': (15,80),\n",
    "    'efficientnet-lite2': (5,25),\n",
    "    'resnet18':(10,50),\n",
    "    'resnet34':(10,50),\n",
    "    'resnet50':(10,50),\n",
    "    'resnet101':(10,50),\n",
    "    'resnet152':(10,50)\n",
    "})\n",
    "label_smoothing = ({           # a = 0.00     a = 0.05     a = 0.10     a = 0.15     a = 0.20\n",
    "    'efficientnet-b0':0.05,    #\n",
    "    'efficientnet-b1':0.05,    #\n",
    "    'efficientnet-b2':0.05,    # 0.05\n",
    "    'efficientnet-b3':0.05,    #\n",
    "    'efficientnet-b4':0.05,    #\n",
    "    'efficientnet-b5':0.05,\n",
    "    'efficientnet-b6':0.05,\n",
    "    'efficientnet-b7':0.05,\n",
    "    'efficientnet-lite0': 0.05,\n",
    "    'efficientnet-lite2': 0.05,\n",
    "    'resnet18':0.05,\n",
    "    'resnet34':0.05,\n",
    "    'resnet50':0.05,\n",
    "    'resnet101':0.05,\n",
    "    'resnet152':0.05\n",
    "})\n",
    "## Testing accuracies (top1,top5)\n",
    "scores = ({\n",
    "    'efficientnet-b0':(0,0),\n",
    "    'efficientnet-b1':(0,0),\n",
    "    'efficientnet-b2':(0,0),\n",
    "    'efficientnet-b3':(0,0),\n",
    "    'efficientnet-b4':(0,0),\n",
    "    'efficientnet-lite0':(0,0),\n",
    "    'efficientnet-lite2': (0,0),\n",
    "    'resnet50':(0,0)\n",
    "})\n",
    "tta_scores = ({\n",
    "    'efficientnet-b0':(0,0),\n",
    "    'efficientnet-b1':(0,0),\n",
    "    'efficientnet-b2':(0,0),\n",
    "    'efficientnet-b3':(0,0),\n",
    "    'efficientnet-b4':(0,0),\n",
    "    'efficientnet-lite0':(0,0),\n",
    "    'efficientnet-lite2': (0,0),\n",
    "    'efficientnet-lite4':(0,0),\n",
    "    'resnet50':(0,0)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Writing training data into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.249071Z",
     "start_time": "2021-11-26T12:31:28.235728Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os.path\n",
    "train_dir = \"scrapped_dataset_new\"\n",
    "training_path = Path(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.288929Z",
     "start_time": "2021-11-26T12:31:28.249683Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filepaths = list(training_path.glob(r'**/*.jpg'))\n",
    "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "images = pd.concat([filepaths, labels], axis=1)\n",
    "train_df = pd.DataFrame(images)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viualization of Training Data, Samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "lbl = train_df['Label']\n",
    "# print(lbl)\n",
    "# print(lbl.size)\n",
    "plt.figure(figsize=(70,20))\n",
    "ax = sns.countplot(x= lbl, data=train_df)\n",
    "ax.bar_label(ax.containers[0], weight = 'bold', fontsize = '30', color = 'red')\n",
    "plt.xticks(rotation=90, fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.yticks( fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.xlabel(\"Labels\", fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.ylabel(\"Number of Images\", fontsize = 40, weight = 'bold', color = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images Per Catergory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.304212Z",
     "start_time": "2021-11-26T12:31:28.290074Z"
    }
   },
   "outputs": [],
   "source": [
    "counted = train_df.groupby([\"Label\"]).size()\n",
    "print(counted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set with 50 Images per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.324789Z",
     "start_time": "2021-11-26T12:31:28.304863Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dir = \"test\"\n",
    "test_path = Path(test_dir)\n",
    "filepaths = list(test_path.glob(r'**/*.jpg'))\n",
    "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "images = pd.concat([filepaths, labels], axis=1)\n",
    "test_df= pd.DataFrame(images)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "lbl = test_df['Label']\n",
    "# print(lbl)\n",
    "# print(lbl.size)\n",
    "plt.figure(figsize=(70,20))\n",
    "ax = sns.countplot(x= lbl, data=test_df)\n",
    "ax.bar_label(ax.containers[0], weight = 'bold', fontsize = '30', color = 'red')\n",
    "plt.xticks(rotation=90, fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.yticks( fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.xlabel(\"Labels\", fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.ylabel(\"Number of Images\", fontsize = 40, weight = 'bold', color = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Fastai and pytorch Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.609896Z",
     "start_time": "2021-11-26T12:31:28.325404Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai import __version__\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from fastai.vision import *\n",
    "from fastai import optimizer, losses, metrics\n",
    "from functools import partial, wraps\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastai version checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "fastai.__version__\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch version checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.device(0)\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # batch size\n",
    "# bs = 128\n",
    "# # image resolution\n",
    "# img_size = 224\n",
    "model = EfficientNet.from_pretrained(\"efficientnet-b0\", num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of data for learner (Fastai learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tfms = [Zoom(),Rotate(), Flip(), Brightness(), Contrast(), Saturation()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs   = batch_size[architecture]\n",
    "imgs = effnet_size[architecture]\n",
    "resize = (imgs*4)//3\n",
    "data0 = (ImageDataLoaders.from_df(train_df, valid_pct=0.2,bs=bs,label_col=1,\n",
    "                                        shuffle_train=True,\n",
    "                                        item_tfms=Resize(imgs),\n",
    "                                        batch_tfms = batch_tfms\n",
    "                                 ))\n",
    "\n",
    "print(\"Image size=\", imgs)\n",
    "print(\"Batch size=\", bs)\n",
    "print(\"Architecture=\", architecture)\n",
    "print(resize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = losses.CrossEntropyLossFlat()\n",
    "model = EfficientNet.from_pretrained(architecture, num_classes=10)\n",
    "learner_type = Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:30.444803Z",
     "start_time": "2021-11-26T12:31:30.385475Z"
    }
   },
   "outputs": [],
   "source": [
    "best_pth = 'tf_exp0_scrapped_dataset_b0'\n",
    "checkpoints = SaveModelCallback(fname=best_pth,monitor='accuracy',comp=np.greater, with_opt=True)\n",
    "# brainfreeze = BnFreeze()\n",
    "learn = ( learner_type(data0, model,metrics=[accuracy],loss_func=loss_func,\n",
    "                        cbs=[ShowGraphCallback(),checkpoints]).to_fp16())\n",
    "print(\"Best pth is=\", best_pth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we choose learning rates?\n",
    "If the learning rate is too slow, training will take a lot of time. If the learning rate is too high, \n",
    "we will be jumping around the minimum loss, getting farther and farther from the minimum and never reach it.\n",
    "So to start the experiment of finding a learning rate, we train the model while increasing the learning rate.\n",
    "Then we plot the loss against the learning rate, and stop when the loss starts to explode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot_lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_lr = 1e-4\n",
    "lr = 1e-3\n",
    "learn.fit_one_cycle(1 , lr_max = slice(low_lr, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfreeze the Learner\n",
    "fast.ai freezes the pre-trained part when we create the model. So up to this point, \n",
    "we only trained the last layer block. Next, we will unfreeze the pre-trained part and train the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_lr = 1e-4\n",
    "lr = 1e-3\n",
    "learn.fit_one_cycle(1, lr_max = slice(low_lr, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test(%) on Test set 1 (50 Images per class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = (ImageDataLoaders.from_df(test_df, valid_pct = 0.0, splitter=None, shuffle=False, label_col=1, item_tfms=Resize(imgs)))\n",
    "preds = learn.get_preds(dl=data_test)\n",
    "preds\n",
    "print(\"length of preds[1]\",len(preds[1]))\n",
    "acc= accuracy(preds[0], preds[1])\n",
    "print(\" BaselineTop-1 Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = {}\n",
    "log_preds, y  = learn.tta(dl=data_test)\n",
    "tta_acc = accuracy(log_preds, y)\n",
    "print(tta_acc)\n",
    "err[0] = (100.0, 100.0*(1.0-float(tta_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_test = ClassificationInterpretation.from_learner(learn, dl =data_test)\n",
    "inter_test.plot_confusion_matrix(figsize = (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=\"models/tf_exp0_scrapped_dataset.p\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving best pth with baseline for future using and for scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "source = 'models/'+best_pth+'.pth'\n",
    "destination = 'models/baseline/'+best_pth\n",
    "# Copy the file from source to destination\n",
    "shutil.copy(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.vision.widgets import ImageClassifierCleaner\n",
    "import random as rn\n",
    "img_size = imgs\n",
    "drop_idxx = []\n",
    "los = []\n",
    "top_losses = []\n",
    "torch.manual_seed(42)\n",
    "def class_frequencies(data,class_key='Label'):\n",
    "    classes = set(data[class_key])\n",
    "    class_freq = {}\n",
    "    max_freq = 0.5\n",
    "    min_freq =10000000#1000000\n",
    "    for cl in classes:\n",
    "        class_df = data[data[class_key]==cl]\n",
    "        freq = len(class_df)\n",
    "        max_freq = max(freq,max_freq)\n",
    "        min_freq = min(freq,min_freq)\n",
    "        class_freq[cl] = freq\n",
    "        #print(f'Frequency of class {cl}: {class_freq[cl]} ')\n",
    "    print(f'Max frequency is {max_freq}')\n",
    "    print(f'Min frequency is {min_freq}')\n",
    "    return class_freq,max_freq,min_freq\n",
    "    \n",
    "\n",
    "def russian_roulette(idx, losses, data,class_key='Label', min_prob=0.5,max_prob=0.9): #min_prob=0.5,max_prob=0.9 original values\n",
    "    \n",
    "    class_freq,max_freq,min_freq = class_frequencies(data,class_key)\n",
    "    min_prob = float(min_freq/max_freq)*max_prob\n",
    "    print(f'Min probability = {min_prob}')\n",
    "    deltap = (max_prob - min_prob)/(max_freq**2)\n",
    "#     deltap = 0.0\n",
    "#     if max_prob > min_prob:\n",
    "#         deltap = (max_prob - min_prob)/(max_freq-min_freq)\n",
    "            \n",
    "    \n",
    "    drop_idx = []\n",
    "    for n,i in enumerate(idx.numpy()):\n",
    "        l = data.iloc[i][class_key]\n",
    "#         print(\"+++++++++ L\", l)\n",
    "#         prob = min_prob + deltap * (class_freq[l]**2)\n",
    "        prob = min_prob + (class_freq[l]-min_freq)*deltap\n",
    "        r = rn.random()\n",
    "#         print(\"rrr\",r, \"prob+++\", prob)\n",
    "        if  r < prob:\n",
    "            drop_idx.append(i)\n",
    "            class_freq[l] -= 1 \n",
    "#     print(class_freq)# returns number of images per class\n",
    "    return drop_idx\n",
    "\n",
    "def data_definition(data,img_size,tfms,model,best_pth,k_samples=100, min_prob=0.4, max_prob = 0.9, largest=True): # k_sample = 1000\n",
    "   \n",
    "    databunch = ( ImageDataLoaders.from_df(data, valid_pct=0.0, bs=bs,label_col=1,\n",
    "                                        shuffle_train=True,\n",
    "                                        item_tfms=Resize(imgs),\n",
    "                                        batch_tfms = batch_tfms))\n",
    "\n",
    "#     \n",
    "    learn_cln = (learner_type(databunch, model,metrics=[accuracy],loss_func=loss_func, cbs=[ShowGraphCallback(),checkpoints]).to_fp16())\n",
    "    learn_cln.load(best_pth)\n",
    "    entire_training_set_csv0 = (ImageDataLoaders.from_df(data, valid_pct = 0.0, splitter=None, shuffle=False, label_col=1, item_tfms=Resize(imgs)))\n",
    "#     entire_training_set_learner0 = (learner_type(entire_training_set_csv0, model, metrics=[accuracy]).to_fp16())\n",
    "#     entire_training_set_learner0.load(best_pth)\n",
    "    interp = ClassificationInterpretation.from_learner(learn_cln, dl = entire_training_set_csv0)\n",
    "    interp.plot_top_losses(100, figsize = (11,15))\n",
    "    losses = None\n",
    "    idx = None\n",
    "    if hybrid:\n",
    "        l_big,idx_big = interp.top_losses(k_samples//2,largest=True)\n",
    "        l_sm,idx_sm = interp.top_losses(k_samples//2,largest=False)\n",
    "        losses = torch.cat((l_big,l_sm),0)\n",
    "        idx = torch.cat((idx_big,idx_sm),0)\n",
    "    else:\n",
    "        losses,idx = interp.top_losses(k_samples,largest=largest)\n",
    "    drop_idx = russian_roulette(idx,losses,data,min_prob = min_prob, max_prob = max_prob)\n",
    "    top_losses = pd.DataFrame(data.iloc[drop_idx])\n",
    "    top_losses = top_losses.to_csv('top_losses'+str(n+1)+'.csv', index = False)\n",
    "    data_filtered = data.drop(data.index[drop_idx])\n",
    "    print(f'Dropped {len(drop_idx)} top losses')\n",
    "    return data_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "### Gym training scheme --> \n",
    "### Threek kind of programs: mass, power or definition\n",
    "\n",
    "drop_idxx = []\n",
    "los = []\n",
    "top_losses = []\n",
    "n_training_series = 10\n",
    "min_epochs = 1\n",
    "min_prob=0.5\n",
    "max_prob=0.9\n",
    "delta_epochs_per_serie =  0\n",
    "min_lr = 1e-4\n",
    "low_lr = 1e-4\n",
    "lr = 1e-3\n",
    "kappa_lr = 1 \n",
    "k_samples = 500\n",
    "data_per_serie = {}\n",
    "data_per_serie[0] = train_df\n",
    "largest = True\n",
    "from_scratch = False\n",
    "hybrid = False\n",
    "cleaning_cycles = 3\n",
    "\n",
    "\n",
    "# First program: slimming --> Reducing data weight, increasing number of epochs, and decreaseing learning rate\n",
    "for c in range(cleaning_cycles):  \n",
    "    for n in range(n_training_series):\n",
    "        print(f'+++++ Training serie: inverse pyramid {n} ++++++++')\n",
    "        epochs_per_serie = min_epochs + n * delta_epochs_per_serie\n",
    "#         lr = min_lr * kappa_lr * (n_training_series - n)\n",
    "#         print(\"dynamic LR\", dynamic_lr)\n",
    "        data_per_serie[n+1] = data_definition(data_per_serie[n],img_size,batch_tfms,model,best_pth,k_samples=k_samples, min_prob = min_prob, max_prob=max_prob)\n",
    "        print(\"************Round: \",n)\n",
    "        print(\"************data per serie:******************\")\n",
    "    #     data_per_serie[n+1]\n",
    "        print(\"************length of data per serie:******************\")\n",
    "        print(len(data_per_serie[n+1]))\n",
    "        \n",
    "        databunch = ( ImageDataLoaders.from_df(data_per_serie[n+1], valid_pct = 0.2,bs=bs,label_col=1,\n",
    "                                        shuffle_train=True,\n",
    "                                        item_tfms=Resize(imgs),\n",
    "                                        batch_tfms = batch_tfms))\n",
    "\n",
    "    #     learn = ( learner_type(databunch, model,metrics=[accuracy,top_5],loss_func=loss,cbs=[ShowGraphCallback(),history]).to_fp16())\n",
    "        learn =(learner_type(databunch, model,metrics=[accuracy],loss_func=loss_func,\n",
    "                            cbs=[ShowGraphCallback(),checkpoints]).to_fp16())\n",
    "#         learn.split( lambda m: (model._conv_head) )\n",
    "        learn.unfreeze() \n",
    "        learn.load(best_pth)\n",
    "        learn.fit_one_cycle(epochs_per_serie, lr_max = slice(low_lr, lr))\n",
    "        if n == 4 or n == 9:\n",
    "            log_preds, y = learn.tta(dl=data_test)\n",
    "            print(\"************Round: \",n)\n",
    "            tta_acc = accuracy(log_preds, y)\n",
    "            print('TTA Accuracy', tta_acc)\n",
    "            err[c*n_training_series + n] = ( 100.0 *float(1.0 - len(data_per_serie[n+1])/len(data_per_serie[0])) ,  100.0 * (1.0 - float(tta_acc)))\n",
    "            print(f'cycle {c} Error Rate is [{err}]')\n",
    "            inter_test = ClassificationInterpretation.from_learner(learn, dl =data_test)\n",
    "            inter_test.plot_confusion_matrix(figsize = (10,10))\n",
    "            PATH=\"models/tropical_fishes_b0_\"+str(c)+\"_\"+str(n)+\".p\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            csv = data_per_serie[n+1].to_csv(\"tropical_fishes_clean_csv\"+str(c)+\"_\"+str(n)+\".csv\", index = False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "for i in range(21):\n",
    "    train_df = data_per_serie[i]\n",
    "    make_csvs = train_df.to_csv('cleaned_csv_'+str(i)+'.csv', index = False)\n",
    "    lbl = train_df['Label']\n",
    "    plt.figure(figsize=(70,20))\n",
    "    ax = sns.countplot(x= lbl, data=train_df)\n",
    "    ax.bar_label(ax.containers[0], weight = 'bold', fontsize = '30', color = 'red')\n",
    "    plt.xticks(rotation=90, fontsize = 40, weight = 'bold', color = 'black')\n",
    "    plt.yticks( fontsize = 40, weight = 'bold', color = 'black')\n",
    "    plt.xlabel(\"Labels\", fontsize = 40, weight = 'bold', color = 'black')\n",
    "    plt.ylabel(\"Number of Images\", fontsize = 40, weight = 'bold', color = 'black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Food101_Kaggle.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
